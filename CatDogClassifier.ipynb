{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jxdn/catvdog/blob/main/CatDogClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6Bqsee5B9Om",
        "outputId": "169d800e-cd02-4938-e8ac-833a7cf06ffe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!kaggle datasets download -d salader/dogs-vs-cats -p \"/content/drive/MyDrive/datasets\"\n",
        "#import zipfile\n",
        "#zip_ref = zipfile.ZipFile('/content/drive/MyDrive/datasets/dogs-vs-cats.zip', 'r')\n",
        "#zip_ref.extractall('/content/drive/MyDrive/datasets/')\n",
        "#zip_ref.close()"
      ],
      "metadata": {
        "id": "_IwnGFVoCnZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GQ59gJ7hcNKD"
      },
      "cell_type": "code",
      "source": [
        "from os import walk\n",
        "import time\n",
        "import numpy as np\n",
        "import PIL.Image as Im #Image Processing library\n",
        "import tqdm #Loading Bar"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gEg7K1qegRlM"
      },
      "cell_type": "markdown",
      "source": [
        "# Section 1: Opening and normalizing images"
      ]
    },
    {
      "metadata": {
        "id": "1fsxsQV3cNKI"
      },
      "cell_type": "markdown",
      "source": [
        "This function gets a folder name and returns the names of all the files in that folder"
      ]
    },
    {
      "metadata": {
        "id": "hcPlW90ecNKJ"
      },
      "cell_type": "code",
      "source": [
        "def imagePaths(folderName):\n",
        "    f = []\n",
        "    for root, dirs, files in walk(folderName):\n",
        "        for filename in files:\n",
        "            f.append(folderName+'/'+filename)\n",
        "    return f"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VnOH6x-FcNKM"
      },
      "cell_type": "markdown",
      "source": [
        "This function opens the images and returns an array of grayscale images"
      ]
    },
    {
      "metadata": {
        "id": "E8J0zhXGcNKN"
      },
      "cell_type": "code",
      "source": [
        "def openImages(paths):\n",
        "    images = []\n",
        "    for path in paths[:numberOfImagesForTraining]:\n",
        "        #print(path)\n",
        "        image = Im.open(path).convert(\"L\")\n",
        "        images.append(image)\n",
        "    return images"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BssaJJz9cVZl"
      },
      "cell_type": "markdown",
      "source": [
        "This function resizes the input images"
      ]
    },
    {
      "metadata": {
        "id": "q2Y2qR-PcNKT"
      },
      "cell_type": "code",
      "source": [
        "def ResizeAndKeepRatio(images,ratio,avgWidth,avgHeight):\n",
        "    newImages = []\n",
        "    sizes = []\n",
        "    # Make all images of the average width while keeping the aspect ratio\n",
        "    for image in images:\n",
        "        if (image.height/image.width)<ratio:\n",
        "            wpercent = (avgWidth / float(image.size[0]))\n",
        "            hsize = int((float(image.size[1]) * float(wpercent)))\n",
        "            image = image.resize((avgWidth, hsize), Im.Resampling.LANCZOS)\n",
        "        else:\n",
        "            hpercent = (avgHeight / float(image.size[1]))\n",
        "            wsize = int((float(image.size[0]) * float(hpercent)))\n",
        "            image = image.resize((wsize, avgHeight),  Im.Resampling.LANCZOS)\n",
        "        newImages.append(image)\n",
        "        sizes.append(image.size)\n",
        "    images = newImages\n",
        "    return images"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z5AGKjdPcNKX"
      },
      "cell_type": "markdown",
      "source": [
        "Get the max height of the images and fill images under that size with black bars to match the heights\n"
      ]
    },
    {
      "metadata": {
        "id": "fQASrvaocNKY"
      },
      "cell_type": "code",
      "source": [
        "def fillImagesWithBlackBars(images,avgWidth,avgHeight):\n",
        "    newImages = []\n",
        "    for image in images:\n",
        "        oldSize = image.size\n",
        "        newSize = (avgWidth, avgHeight)\n",
        "        newImage = Im.new(\"L\", newSize,color=128)\n",
        "        newImage.paste(image, (int((newSize[0] - oldSize[0]) / 2),\n",
        "                                int((newSize[1] - oldSize[1]) / 2)))\n",
        "        newImages.append(newImage)\n",
        "    images = newImages\n",
        "    return images"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oet7MWdLcNKb"
      },
      "cell_type": "markdown",
      "source": [
        "This function takes images of different sizes and returns images of the same size without distortion"
      ]
    },
    {
      "metadata": {
        "id": "PFOBMijLcNKc"
      },
      "cell_type": "code",
      "source": [
        "def adjustImageSizes(images,size = (0,0)):\n",
        "    # Get average Width of images if we don't have it yet (training set)\n",
        "    if (size==(0,0)):\n",
        "        sizes = []\n",
        "        for image in images:\n",
        "            sizes.append(image.size)\n",
        "        averages = np.ceil(np.mean(sizes, axis=0))\n",
        "\n",
        "        avgWidth = int(averages[0])\n",
        "        avgHeight = int(averages[1])\n",
        "    # Otherwise, this would be the test set in which case we apply the same dimensions for the\n",
        "    # training set\n",
        "    else:\n",
        "        avgWidth = size[0]\n",
        "        avgHeight = size[1]\n",
        "    ratio = avgHeight / avgWidth\n",
        "\n",
        "    images = ResizeAndKeepRatio(images,ratio,avgWidth,avgHeight)\n",
        "\n",
        "    images = fillImagesWithBlackBars(images,avgWidth,avgHeight)\n",
        "    return images"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6xrnhd51cNKh"
      },
      "cell_type": "markdown",
      "source": [
        "Get labels of the images from the paths"
      ]
    },
    {
      "metadata": {
        "id": "AnnAGqsccNKi"
      },
      "cell_type": "code",
      "source": [
        "def getLabels(paths):\n",
        "    labels = []\n",
        "    for path in paths[:numberOfImagesForTraining]:\n",
        "        if \"cat\" in path:\n",
        "            labels.append(1)\n",
        "        else:\n",
        "            labels.append(0)\n",
        "    return labels"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ixowgZrggZWv"
      },
      "cell_type": "markdown",
      "source": [
        "Converts an image to an array of numbers; those numbers\n",
        "represent the pixels"
      ]
    },
    {
      "metadata": {
        "id": "qhNdD8YmcNKo"
      },
      "cell_type": "code",
      "source": [
        "def imagesToArray(images):\n",
        "    array = []\n",
        "    for image in images:\n",
        "        element = np.asarray(image)\n",
        "        element = element.flatten()\n",
        "        element = normalize(element)\n",
        "        array.append(element)\n",
        "    return array"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "umDr_JuOcNKr"
      },
      "cell_type": "markdown",
      "source": [
        "Process train images by making them the same size and extracting their labels, then turning them to"
      ]
    },
    {
      "metadata": {
        "id": "MpvNSEhncNKs"
      },
      "cell_type": "code",
      "source": [
        "def processTrainImages(path):\n",
        "    paths = imagePaths(path)\n",
        "    labels = getLabels(paths)\n",
        "    images = openImages(paths)\n",
        "    images = adjustImageSizes(images)\n",
        "    X = imagesToArray(images)\n",
        "    return X,labels,images[0].size"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-fqUfiV1gg_h"
      },
      "cell_type": "markdown",
      "source": [
        "We need a seperate function to process the test images because their width and height is not determined by the average width and height of test images but by the average width and height of the ttraining images.\n",
        "This is because the number of inputs into the model needs to be the same whether we are training or testing."
      ]
    },
    {
      "metadata": {
        "id": "7xGiPcv4cNKv"
      },
      "cell_type": "code",
      "source": [
        "def processTestImages(path,size):\n",
        "    paths = imagePaths(path)\n",
        "    images = openImages(paths)\n",
        "    images = adjustImageSizes(images,size)\n",
        "    X = imagesToArray(images)\n",
        "    return X,images"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QiE-efm5cNKy"
      },
      "cell_type": "markdown",
      "source": [
        "# Section 2: The learning"
      ]
    },
    {
      "metadata": {
        "id": "cw5_thh6cNKz"
      },
      "cell_type": "markdown",
      "source": [
        "Activation functions and their derivatives"
      ]
    },
    {
      "metadata": {
        "id": "DKs1gsN1cNK0"
      },
      "cell_type": "code",
      "source": [
        "def RelU(Z):\n",
        "    return np.maximum(Z,0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WN9-lZbycNK3"
      },
      "cell_type": "code",
      "source": [
        "def RelUPrime(Z):\n",
        "    return np.where(Z > 0, 1, 0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FmN8jZ_NcNK7"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(S):\n",
        "    sig = 1/(1+np.exp(-S))\n",
        "    return sig"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ptECIPEIcNLB"
      },
      "cell_type": "code",
      "source": [
        "def sigmoidPrime(S):\n",
        "    sig = sigmoid(S)*(1-sigmoid(S))\n",
        "    return sig"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2fMSgcaHcNLH"
      },
      "cell_type": "markdown",
      "source": [
        "Normalization"
      ]
    },
    {
      "metadata": {
        "id": "XRAXClticNLI"
      },
      "cell_type": "code",
      "source": [
        "# Old normalization function, does not give good results\n",
        "def oldNormalize(X):\n",
        "    sum = np.sum(np.square(X))\n",
        "    sqr = sum**(0.5)\n",
        "    X = X/sqr\n",
        "    return X"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4EpjFhM0cNLL"
      },
      "cell_type": "code",
      "source": [
        "def normalize(X):\n",
        "    max = np.max(X)\n",
        "    min = np.min(X)\n",
        "    norm = (X-min)/(max-min)\n",
        "    return norm"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8j5u3mADcxai"
      },
      "cell_type": "markdown",
      "source": [
        "Initializition of weights and biases"
      ]
    },
    {
      "metadata": {
        "id": "zDj4H5YMcNLR"
      },
      "cell_type": "code",
      "source": [
        "def initialize(nx):\n",
        "    W = []\n",
        "    B = []\n",
        "    # Factor by which the values are shrunk\n",
        "    factor = 1\n",
        "    firstW = np.random.randn(nodes,nx)*factor\n",
        "    W.append(firstW)\n",
        "    firstB = np.random.randn(nodes, 1) *factor\n",
        "    B.append(firstB)\n",
        "    for i in range(L-2):\n",
        "        middleW = np.random.randn(nodes,nodes)*factor\n",
        "        W.append(middleW)\n",
        "        middleB = np.random.randn(nodes,1)*factor\n",
        "        B.append(middleB)\n",
        "    lastW = np.random.randn(1,nodes)*factor\n",
        "    W.append(lastW)\n",
        "    lastB = np.random.randn() *factor\n",
        "    B.append(lastB)\n",
        "\n",
        "    return W,B"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aixcZ3WKcNLV"
      },
      "cell_type": "markdown",
      "source": [
        "Gradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "BRaPCTLqcNLX"
      },
      "cell_type": "code",
      "source": [
        "def gradientDescent(W,B,X,Y,iters,learnRate):\n",
        "    X = np.transpose(X)\n",
        "    nx,m = np.shape(X)\n",
        "    Z = [None]*L\n",
        "    A = [None]*L\n",
        "    dZ = [None]*L\n",
        "    dW = [None]*L\n",
        "    dB = [None]*L\n",
        "    dA = [None]*L\n",
        "\n",
        "    for j in tqdm.tqdm(range(iters)):\n",
        "        # Forward Pass\n",
        "        A,Z = forwardPass(X,W,B,Z,A)\n",
        "        # Initialization for backward propagation\n",
        "        dZ[L-1] = A[L-1] - Y\n",
        "        dA[L-2] = np.dot(np.transpose(W[L-1]),dZ[L-1])\n",
        "        dW[L-1] = (1 / m) * np.dot(dZ[L-1], A[L-2].T)\n",
        "        dB[L-1] = (1 / m) * np.sum(dZ[L-1])\n",
        "        # Back Propagation\n",
        "        for layer in range(L-2, -1, -1):\n",
        "            dZ[layer] = np.multiply(dA[layer],sigmoidPrime(Z[layer]))\n",
        "            dW[layer] = (1/m) * np.dot(dZ[layer],A[layer-1].T)\n",
        "            dB[layer] = (1/m) * np.sum(dZ[layer],axis=1,keepdims=True)\n",
        "            dA[layer-1] = np.dot(np.transpose(W[layer]),dZ[layer])\n",
        "        for layer in range(0,L):\n",
        "            W[layer] = W[layer] - learnRate * dW[layer]\n",
        "            B[layer] = B[layer] - learnRate * dB[layer]\n",
        "    return W, B"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rIyH1UatcNLZ"
      },
      "cell_type": "code",
      "source": [
        "def forwardPass(X,W,B,Z,A):\n",
        "    layer = 0\n",
        "    Z[layer] = np.dot(W[layer], X) + B[layer]\n",
        "    A[layer] = sigmoid(Z[layer])\n",
        "    for layer in range(1, L):\n",
        "        Z[layer] = np.dot(W[layer], A[layer - 1]) + B[layer]\n",
        "        A[layer] = sigmoid(Z[layer])\n",
        "    # Overwriting A[L-1] instead of checking in every iteration\n",
        "    A[L - 1] = sigmoid(Z[L - 1])\n",
        "    return A,Z"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MAFbn1RGhRMO"
      },
      "cell_type": "markdown",
      "source": [
        "test() returns the results instead of the success rate. This is because this is a learning project and it is helpful for me to see the result outputed for each element of the test set ad therefore see where learning is good and where it is bad."
      ]
    },
    {
      "metadata": {
        "id": "mE7Bhnn9cNLc"
      },
      "cell_type": "code",
      "source": [
        "def test(X,W,B):\n",
        "    X = np.transpose(X)\n",
        "    result = []\n",
        "    A,Z = forwardPass(X,W,B,[None]*L,[None]*L)\n",
        "    for element in A[L-1][0]:\n",
        "        result.append(round(element))\n",
        "    return result"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0q6_GMELdvmt"
      },
      "cell_type": "markdown",
      "source": [
        "## Main Function"
      ]
    },
    {
      "metadata": {
        "id": "QC9DUNAicNLg"
      },
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    trainPath = \"/content/drive/MyDrive/datasets/alldata/train/cats\"\n",
        "    testPath  = \"/content/drive/MyDrive/datasets/alldata/test/cats\"\n",
        "    X,Y,size = processTrainImages(trainPath)\n",
        "    W,B = initialize(np.shape(X)[1])\n",
        "\n",
        "    print(\"Learning with a learning rate of\",learningRate,\"and\",iterations,\"iterations\")\n",
        "    W,B = gradientDescent(W,B,X,Y,iterations,learningRate)\n",
        "    X,images = processTestImages(testPath, size)\n",
        "    result = test(X,W,B)\n",
        "    print(\"Result\",result)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TOuRCIp3do9a"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "b0ivGx8XcNLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1d10fc-77e0-468d-ae5d-02990debca19"
      },
      "cell_type": "code",
      "source": [
        "# This is the number of images used for training\n",
        "# make it larger to train better\n",
        "# make it smaller to test quickly\n",
        "# make sure the number does not exceed the number of training images you have\n",
        "numberOfImagesForTraining = 1000\n",
        "# Number of Layers\n",
        "L = 10\n",
        "# Number of nodes per hidden layer\n",
        "nodes = 15\n",
        "# Number of Iterations\n",
        "iterations = 500\n",
        "# Learning rate\n",
        "learningRate = 0.1\n",
        "\n",
        "#main()\n",
        "trainPath = \"/content/drive/MyDrive/datasets/custom/train\"\n",
        "testPath  = \"/content/drive/MyDrive/datasets/custom/test\"\n",
        "X,Y,size = processTrainImages(trainPath)\n",
        "W,B = initialize(np.shape(X)[1])\n",
        "print(\"Learning with a learning rate of\",learningRate,\"and\",iterations,\"iterations\")\n",
        "W,B = gradientDescent(W,B,X,Y,iterations,learningRate)\n",
        "X,images = processTestImages(testPath, size)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning with a learning rate of 0.1 and 500 iterations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]<ipython-input-15-3370bb43d581>:2: RuntimeWarning: overflow encountered in exp\n",
            "  sig = 1/(1+np.exp(-S))\n",
            "100%|██████████| 500/500 [03:34<00:00,  2.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testPath  = \"/content/drive/MyDrive/datasets/custom/test\"\n",
        "X,images = processTestImages(testPath, size)\n",
        "result = test(X,W,B)\n",
        "print(\"Result\",result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y9fhVkqYsH1",
        "outputId": "a3375afb-d988-4d40-efec-a9ae8fa7f7f3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-3370bb43d581>:2: RuntimeWarning: overflow encountered in exp\n",
            "  sig = 1/(1+np.exp(-S))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQpXVIvKaRMk",
        "outputId": "ccfbd2f1-20da-4634-85d6-6617aff896e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    }
  ]
}